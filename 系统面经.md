# 系统面经

### web开发安全

- [web开发安全](#toc_0)
- [主流web漏洞](#toc_1)
- [csrf 攻击](#toc_2)
- [xss 攻击](#toc_3)
- [SQL 注入](#toc_4)
- [URL跳转漏洞](#toc_5)
- [什么是DoS、DDoS、DRDoS攻击？如何防御？](#toc_6)

------

#### 主流web漏洞

![img](https://cdn.nlark.com/yuque/0/2019/png/406432/1565344289364-8c9fc4f0-7445-446d-a8d5-0cdb76599735.png)

#### csrf 攻击

CSRF（Cross-site request forgery）跨站请求伪造，也被称为“One Click Attack”或者Session Riding，通常缩写为CSRF或者XSRF，是一种对网站的恶意利用。但它与XSS非常不同，XSS利用站点内的信任用户，而CSRF则通过伪装成受信任用户的请求来利用受信任的网站。与XSS攻击相比，CSRF攻击往往不大流行（因此对其进行防范的资源也相当稀少）和难以防范，所以被认为比XSS更具危险性。

[浅谈CSRF攻击方式](https://www.cnblogs.com/hyddd/archive/2009/04/09/1432744.html)

![img](https://cdn.nlark.com/yuque/0/2019/png/406432/1565344298809-05ddd5cc-6dd6-4c60-bda9-a45919a192d1.png)

#### xss 攻击

[Cross-Site Scripting](https://segmentfault.com/a/1190000016551188)（跨站脚本攻击）简称 XSS，是一种代码注入攻击。攻击者通过在目标网站上注入恶意脚本，使之在用户的浏览器上运行。利用这些恶意脚本，攻击者可获取用户的敏感信息如 Cookie、SessionID 等，进而危害数据安全。

- 在 HTML 中内嵌的文本中，恶意内容以 script 标签形成注入。
- 在内联的 JavaScript 中，拼接的数据突破了原本的限制（字符串，变量，方法名等）。
- 在标签属性中，恶意内容包含引号，从而突破属性值的限制，注入其他属性或者标签。
- 在标签的 href、src 等属性中，包含 javascript: 等可执行代码。
- 在 onload、onerror、onclick 等事件中，注入不受控制代码。
- 在 style 属性和标签中，包含类似 background-image:url("javascript:..."); 的代码（新版本浏览器已经可以防范）。
- 在 style 属性和标签中，包含类似 expression(...) 的 CSS 表达式代码（新版本浏览器已经可以防范）。

XSS 分类

| 类型       | 存储区*                 | 插入点*         |
| ---------- | ----------------------- | --------------- |
| 存储型 XSS | 后端数据库              | HTML            |
| 反射型 XSS | URL                     | HTML            |
| DOM 型 XSS | 后端数据库/前端存储/URL | 前端 JavaScript |

#### SQL 注入

#### URL跳转漏洞

#### 什么是DoS、DDoS、DRDoS攻击？如何防御？





#### 消息中间件

目前在使用的消息中间件是RocketMQ和kafka



- Rocket 特点

RocketMQ是一个分布式消息和流处理平台，具有低延迟，高性能和高可靠性，亿万级容量和灵活的可扩展性。它由四部分组成：名称服务器，代理服务器，生产者和消费者。它们中的每一个都可以水平扩展，而不会出现单点故障。



- Kafka 特点

支持消息的发布和订阅，这个和RocketMq类似；支持数据实时处理；能保证消息的可靠性投递；支持消息的持久化存储，并通过多副本分布式的存储方案来保证消息的容错；高吞吐率，单 Broker 可以轻松处理数千个分区以及每秒百万级的消息量。



- Rocket和Kafka的区别

| 比较项                 | RocketMq                        | Kafka                             |
| ---------------------- | ------------------------------- | --------------------------------- |
| **顺序消息**           | 确保分区内的消息的顺序          | 确保严格的消息顺序,并能优雅的扩展 |
| **定时消息**           | 不支持                          | 支持                              |
| **批量消息**           | 支持,异步生产                   | 支持,使用同步的模式以避免消息丢失 |
| **广播消息**           | 不支持                          | 支持                              |
| **消息过滤**           | 支持, 可以使用Kafka流来筛选消息 | 支持,基于SQL92的属性筛选表达式    |
| **服务器端触发的重试** | 不支持                          | 支持                              |
| **消息存储**           | 高性能存储                      | 高性能和低延迟文件存储            |
| **消息可追溯**         | 支持                            | 支持,时间戳和偏移量               |
| **消息优先级**         | 不支持                          | 不支持                            |
| **管理和操作工具**     | 支持,通过终端命令可以进行配置   | 开箱即用, 用户只需要注意一些配置  |
|                        |                                 |                                   |



- 具体指导

中台应用间的业务消息都是通过RocketMQ来进行，一般性的应用消息通过RocketMQ来进行，RocketMQ对业务消息提供了更高的可用性、管理方便、使用上可以按Topic进行筛选等特点。

但结合业务特点，比较计量数据， 需要进行高速大批量进行写入和消费，消费方固定，但对实时顺序性等要求没有那么高的可以考虑kafka



#### NoSQL数据库

- 目前中台在使用的NoSQL数据库主要有

1. HBase
2. MongoDB

3. ElasticSearch
4. Redis



- HBase



Hbase，是一个高可靠、高性能、可伸缩的分布式数据库。Hbase参考了谷歌的BigTable建模，使用HDFS作为底层存储。使用Zookeeper作为协同服务组件。



可以提供数据的实时随机读写的NoSQL数据库，他的特点如下：

1.  Hbase的表没有固定的字段定义
2.  Hbase的表中每行存储的都是一些key-value对

3.  Hbase的表中有列族的划分，用户可以指定将哪些kv插入哪个列族
4.  Hbase的表在物理存储上，是按照列族来分割的，不同列族的数据一定存储在不同的文件中

5.  Hbase的表中的每一行都固定有一个行键，而且每一行的行键在表中不能重复
6.  Hbase中的数据，包含行键，包含key，包含value，都是byte[ ]类型，hbase不负责为用户维护数据类型

7.  HBASE对事务的支持很差

HBASE相比于其他nosql数据库(mongodb、redis、cassendra、hazelcast)的特点：

1. Hbase的表数据存储在HDFS文件系统中
2. 存储容量可以线性扩展； 数据存储的安全性可靠性极高！



***如果要使用HBase，对于数据的RowKey怎么设计需要慎重考虑，RowKey的设计会极大的影响后续的业务便利性，而且一段设计定型了，后续也很难更改。\***



- MongoDB







- ElasticSearch

ES特性：是一个**分布式、可扩展、实时的搜索与数据分析引擎**。

1. **全文搜索**
2. **结构化数据的实时统计**

3. **数据分析**
4. **复杂的人类语言处理**

5. **地理位置和对象间关联关系**等



ES特点（有点）：

1. 速度快：当数据量到达千万级以上的时候，关系型数据库单表无论是通过增加索引、分库分表来优化，最终能够优化的效果往往不如人意（且分库分表复杂度较高），而ES可以轻松hold住千万、亿级数据量。
2. 不只是全文检索：在传统关系型数据库中，我们很多使用需要采用模糊查询的方式来获取想要的数据，LIKE '%检索项%'，  SQL不会走索引的（不符合最左前缀原则），将执行全表扫描，性能很差。 但在ES中实现上述的查询则很简单，可以实时的查询到想要的结果。





- Redis

Redis是一个高性能的(key/value)分布式内存数据库，基于内存运行并支持持久化的NoSQL数据库。value的数据类型支持主要有 String、List、Hash、Set、ZSet 这5种。



Redis的应用场景主要有:

1. String

1. 1. 取最新N个数据的操作如：可以将最新的10条评论的ID放在Redis的List集合里面
   2. 模拟类似于HttpSession这种需要设定过期时间的功能

1. 1. 缓存： Redis提供了键过期功能，也提供了灵活的键淘汰策略
   2. 计数器：什么是计数器，如电商网站商品的浏览量、视频网站视频的播放数等

1. 1. 分布式锁 ： 可以利用Redis的setnx功能来编写分布式的锁，如果设置返回1说明获取锁成功，否则获取锁失败
   2. 分布式Session

1. 1. 限流

1. Hash

1. 1. 存储用户信息
   2. 用户主页访问量

1. 1. 组合查询

1. List：微博关注人时间轴列表、简单队列

1. 1. Redis提供的有序集合数据类构能实现各种复杂的排行榜应用

1. Set

1. 1. 赞、踩
   2. 标签

1. 1. 好友关系

1. Zset：排行榜



- 定时任务使用场景

1. 定时备份数据
2. 业务（订单）超时自动取消

3. 业务（订单）失败后的定时重试
4. 按时间段统计信息

5. 业务定期（优惠券要过期）给用户发送提醒消息等。



#### 定时任务



- 定时任务的基本概念

执行器：负责执行任务。

调度器：根据配置（cron表达式）详情，告知执行器去执行任务

任务：自己的业务实现，比如优惠券要过期给用户发送一个提醒。



- 使用xxl-job任务需要注意的地方

1. 定时任务是靠机器按crob表达式进行调度的，都是有一定的的延迟，如果有一个业务要求12:00分生效，理想情况下也会有几秒的延迟，不可能12:00准时生效。
2. xxl-job只是一个触发入口，具体的实现时需要业务来处理的。

3. 对耗时严重的任务，建议仅仅通过xxl-job进行触发，触发后以异步方式执行，不要占用xxl-job的链接资源。
4. 对大批量的任务，最好结合MQ队列一起来处理。



xxl-job接入指南 ：https://qianxun.yuque.com/ddad5m/ykumxg/kgb6h9



#### 缓存中间件



开发过程中经常性的需要对数据进行缓存，常用的缓存技术手段基本有下面几个

1. 无外部依赖的，jdk自身提供的 HashMap/ConcurrentHashMap
2. 单机缓存管理，简单引入jar包的guava

3. 分布式缓存管理，使用redis，memcache等



这里重点强调下使用分布式缓存的选型，为了提高大家的编码效率，编码一致性，缓存的统一处理框架推荐使用JetCache：

https://github.com/alibaba/jetcache

https://github.com/alibaba/jetcache/wiki/Home_CN





#### 性能设计

1. 性能指标分解

  系统性能指标

  子系统性能指标



2. 性能监控

  是否需要性能方面的监控，如过需要，请给出明确的监控指标项。

3. 限流和降级

  限流和降级是否需要，策略，及使用的技术方案。

4. 性能相关设计策略与性能需求的映射关系



5. 架构设计阶段提前做好快速定位性能问题或故障的预案。SkyWalking作为默认需要接入的。

​    对请求的各个环节、链路进行分析，找到可能出现瓶颈的地方，是前端需要优化，还是后端服务需要优化，还是存储需要优化。





# 应用性能设计原则



## 1.考虑使用分布式缓存原则

1.1设计cache集群化部署，集中更新缓存带来性能和资源开销；如果cache失效，大量请求命中DB带来服务性能雪崩，避免丢失过多数据造成服务压力陡增。避免单点问题，提供更加高可用，高性能的服务。

1.2 设计热点数据进行预热加载：高峰期来临前，将热点数据提前存入缓存，提高高峰期的服务性能。

1.3 不存在的数据并定期清理：防止查询不到，导致cache无法命中而频繁访问DB的场景

1.4 分布式缓存集中管理，保证可扩展性，降低系统复杂度



## 2.考虑异步化设计原则

通过分布式消息队列来实现削峰的目的，通过业务配合技术来解决问题。一般合理使用消息队列，可有效抵御促销活动大量涌入的订单对系统造成的冲击



## 3. 考虑集群策略原则

使用负载均衡技术为一个应用构建一个由多台服务器组成的服务器集群，将并发请求分发到多台服务器上处理，避免单一服务器因负载压力过大而响应缓慢，使用户请求具有更好的响应延迟特性



## 4.代码性能优化原则

4.1 多线程中的密集型计算：线程数不宜超过CPU核数。如果是IO处理，则线程数=[任务执行时间/(任务执行时间-IO等待时间)] * CPU核数

4.2将对象设计成无状态对象，多采用局部对象，并发访问资源时使用锁

4.3 资源复用原则：系统运行时，尽量减少那些开销很大的资源的创建和销毁

4.4合理设置JVM参数，以最大程度避免不合理的full gc



## 5.存储性能优化原则

5.1 硬件选择——机械硬盘VS固态硬盘

5.2 B+树 vs LSM树（数据结构）：关系型数据库的索引采用B+树进行实现，nosql数据库采用了LSM树进行存储。所以对于读写操作较多，LSM树其性能远高于b+树

5.3 存储和备份方式选择（RAID vs HDFS），采用HDFS结合map reduce进行海量数据存储和分析，能自动进行并发访问和冗余备份，具有很高的可靠性



# 一致性

在业务系统中，数据一致性非常的重要，总体上说，有2种方式来保证一致性

- 事务性的强一致性
- 数据的最终一致性



- 事务性的强一致性

事务性的强一致，实施的代价比较高，基本局限于服务应用内部，并且一个应用如果事务太多，比如降低并发性，增加数据库的负载；如果在分布式情况下，要实现事务一致性，还需要引入复杂的分布式事务解决方案；所以，不建议一般的业务系统采用事务一致性方案，只对资金类要求很严格的业务有限使用。



- 数据的最终一致性

数据的最终一致性，需要处理好几个问题

1. 业务调用的幂等处理
2. 业务失败的重试（补偿）处理

3. 业务异步处理的握手处理
4. 数据不一致的检测和报警



前3个步骤是保证一致性的关键，需要开发在开发过程中重点把握；第4个步骤是事后止损措施。



要求：

- 明确系统内部哪些数据需要一致
- 明确系统和外部系统之间哪些数据需要保持数据一致性

- 保证数据一致的策略和实现方式



# 可靠性

**I.避免单点（重要）**：集群+LB、容器化部署(k8s的编排管理保证了pod的数量和健康,并且支持弹性伸缩、滚动升级)

**II.熔断**：快速失败，避免服务雪崩，保护整个服务链路；（dubbo+hystrix）

**III.降级**： 不同的业务采用不同的降级逻辑，保证核心服务的正常运行；（在dubbo消费端可以配置mock对调用进行降级）

**IV.限流**： 防止流量超出系统所能承受的阈值；（sentinel）

**V.缓存**： 避免同一时间大量请求直接落到数据库，将数据库击垮；（本地缓存、分布式缓存）

**VI.超时和重试机制**： 设置超时时间避免请求堆积，重试避免偶尔的网络抖动；（配置dubbo的timeout和retry次数，注意服务接口的幂等）



## 一、什么时候需要读写分离？



- 高频读，低频写
- 读请求单节点数据库性能难以支撑（通常每秒QPS吞吐量可能超过3w，IOPS超过2w，建议考虑）

- 读请求有弹性伸缩的要求（是否需要高峰扩容，闲时回收）
- 缓存存在的情况下，仍旧可能因为命中率，首次加载等场景，存在集中查询的场合



## 二、读写分离前提



- 对主从同步导致性能的少量下降有一定的容忍性（性能：同步复制<半同步复制<异步复制）
- 对主从分离的数据可见性延迟有一定的容忍性

- 对业务中读取后更新等场景有严格的把控评估和解决方案
- 对代码中可读写分离的功能有明确的把控

- 事务中查询不应读写分离
- 读写分离可能加剧业务脏写问题，因此从根本上规避此类问题接入读写分离的系统应当有数据写入一致性方案，例如version版本控制



## 三、读写分离方案



1. 阿里云数据库读写分离方案：

https://help.aliyun.com/document_detail/96073.html?spm=a2c4g.11186623.2.13.54582e9bkHFr2j#concept-ptl-fl4-wdb



1. ShardingJDBC及ShardingProxy：

https://shardingsphere.apache.org/document/current/en/features/replica-query/



1. 业务系统实现可控读写分离

自行实现`



## 四、方案比较

|               | 阿里云数据库读写分离                                         | 数据库中间件    | 业务系统实现读写分离                     |
| ------------- | ------------------------------------------------------------ | --------------- | ---------------------------------------- |
| 部署方式      | 仅公有云                                                     | 公有云/专有云   | 公有云/专有云                            |
| 代码耦合      | 无侵入/标记侵入                                              | 无侵入/标记侵入 | 标记侵入/强耦合                          |
| 可控性        | 自动分发：代理层通过SELECT及UPDATE关键字自动分发，业务不可控标记分发：通过force hint标记分发控制，对业务有侵入控制 | 同左            | 业务完全控制，指定本次查询是读库或者写库 |
| 事务          | 不支持事务                                                   | 不支持事务      | 不支持事务                               |
| 可用区/多中心 | 不支持，读写节点不可跨可用区                                 | 支持            | 支持                                     |



## 一、什么时候需要分库分表

- 预估单表记录数大于500w，且可能持续增长
- 预估单库容量大于1TB，且可能持续增长

- 预估单库或者单表存在读写热点问题，尤其是写热点问题
- 预估存在多中心场合



## 二、分库分表的作用

- 水平扩展，理论上数据库容量无限扩展
- 热点分散，合理设计分表，缓解单表中频繁访问，行锁及I/O争抢的问题

- 易于维护，单表DDL可操作，不因为表体积过大导致无法维护
- 提升性能，相较大体积表，即使表上出现skip等耗时操作，仍旧可以等待响应，不至于挂起

- 多中心部署技术路径必须项



## 三、分库分表方案

### 1. 设计分片键及分片策略

1. 1. 考虑最频繁查询业务字段，且全局唯一（真正的全局唯一，支持多中心架构）
   2. 分片策略下，预估单表容量大小

1. 1. 分片策略下，预估单表查询热度



### 2. 动态分片 or 静态分片

|          | 静态分片                                                     | 动态分片                                                   |
| -------- | ------------------------------------------------------------ | ---------------------------------------------------------- |
| 分片算法 | Hash等取模法                                                 | 时间片算法，雪花算法，范围算法，检索表等                   |
| 节点数   | 预设固定                                                     | 执行中可变                                                 |
| 节点变化 | 成本极大，需数据迁移重排                                     | 可适应算法变更                                             |
| 单点查询 | 性能极好                                                     | 根据算法不同，理论上最好情况可达静态分片同等性能           |
| 跨表查询 | 少量分片无需构建索引表分片数较多需要构建索引表或者其他数据整合方案 | 需要构建动态索引表或者其他数据整合方案                     |
| 跨库分片 | 跨库算法和跨表算法可以与业务无关                             | 跨库算法和跨表算法需要根据业务设计                         |
| 维护性   | 较差，无法直观获知数据位于哪个分片中                         | 可设计，可以通过算法将分片信息维护于分片键本身或者检索表中 |



### 3. 跨表业务查询方案设计

需要考虑业务跨表数据连接、查询和排序等场景。设计全局静态索引表，或者数据聚合查询方案（例如ElasticSearch等）



### 4. 维护可行性

- 数据迁移可行性
- 分片重排可行性

- 数据聚合可行性

### 

## 四、可执行参考方案

### 1. 阿里云集群数据库

#### 1.1 PolarDB

PolarDB是阿里云提供的基于计算与存储分离理念设计的原生关系型数据库，其已经提供了大容量存储及读写分离特性，支持最大容量100T，16个计算节点，每节点最高88vCPU（理论值）。

可参考：https://www.aliyun.com/product/polardb?spm=5176.13910061.J_8058803260.128.467830f1cXm7rR



#### 1.2 PolarDB-X（原DRDS，水平分片型）

PolarDB-X是阿里云提供的基于DRDS分布式SQL模型以及X-DB分布式存储设计的云分布式原生关系型数据库，支持PB级存储，峰值8700wTPS（理论值），基于Paxos一致性协议确保跨可用区数据一致性。

可参考：https://www.aliyun.com/product/drds?spm=5176.155538.J_8058803260.129.59baee1fsN3PbW



### 2. 分库分表中间件

#### 2.1 ShardingJDBC 

可参考：https://shardingsphere.apache.org/document/current/en/overview/#shardingsphere-jdbc

#### 2.2 ShardingProxy

可参考：https://shardingsphere.apache.org/document/current/en/overview/#shardingsphere-proxy



|          | shardingJDBC                                     | shardingProxy                          |
| -------- | ------------------------------------------------ | -------------------------------------- |
| 架构     | 分布式无状态设计                                 | 中心式有状态设计                       |
| 性能     | 本地执行，静态算法，性能较好                     | 云服务管理，代理模式，多一跳，性能稍差 |
| 可靠性   | 无单点故障                                       | 有单点故障                             |
| 可运维性 | 节点变更、路由切换存在一致性风险（根据算法决定） | 节点变更代理统一管理，算法保障一致性   |
| 编码     | 引入jar包，配置数据                              | 需要部署独立进程                       |



### 3. 业务定制化分库分表（不推荐）

在一些特殊场合下，可以考虑业务引入定制化分库分表策略。

优点：

1. 1. 数据分布贴近业务，可以最合理设计
   2. 性能可定制

1. 1. 数据可视性较好

缺点：

1. 1. 可运维性差
   2. 需要人力维护成本

1. 1. 后期变更需要完全定制





#### 监控报警



监控事项分为服务监控，业务监控，性能监控。

每类监控需要明确：

- 监控项（服务状态，业务埋点，性能指标等）
- 监控的实现方式（skywalking，metric，日志等）

- 感知方式

- - 定期查看监控页面
  - 报警通知

- - - 报警阈值，通知方式，通知频次等